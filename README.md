# ğŸ‡®ğŸ‡³ Build for Bharat â€” Intelligent Data Analysis Pipeline

A local, agent-based data intelligence system for discovering, retrieving, and analyzing live datasets from https://data.gov.in.  
It powers BharatGPT â€” a fully offline Q&A platform that answers analytical questions using official Indian government datasets.

Built By Veerain Sood
Btech IIT Tirupati CSE
CS22B049
---

## ğŸŒ Overview

### ğŸ”¹ What it does
- Connects directly to data.gov.inâ€™s backend API:
      https://www.data.gov.in/backend/dmspublic/v1/resources
- Builds a local DuckDB + JSON index for sectors such as:
  - Crop Development & Seed Production
  - Research, Education & Biotechnology
  - Temperature and Rainfall
  - PM-KISAN Beneficiaries
- Supports semantic retrieval, dataset reasoning, and step-wise execution using a local LLM.

---

## ğŸ§© Architecture

      Build_For_Bharat/
      â”œâ”€â”€ dataHandlers/
      â”‚   â”œâ”€â”€ connectors/
      â”‚   â”‚   â””â”€â”€ ogdp_scraper.py          â† Pulls datasets metadata from data.gov.in
      â”‚   â”œâ”€â”€ indexer/
      â”‚   â”‚   â”œâ”€â”€ metadata_index.py        â† Builds and merges DuckDB metadata indices
      â”‚   â”‚   â””â”€â”€ dataset_selector.py      â† Handles dataset-family classification
      â”‚   â”œâ”€â”€ llm_tools/
      â”‚   â”‚   â”œâ”€â”€ dataframeFetcher.py      â† Loads datasets into pandas DataFrames
      â”‚   â”‚   â””â”€â”€ ollama_utils.py          â† Manages local model sessions (Ollama)
      â”‚   â”œâ”€â”€ analyzers/
      â”‚   â”‚   â””â”€â”€ runAnaysis.py            â† Executes LLM-generated function sequences
      â”‚   â”œâ”€â”€ agents/
      â”‚   â”‚   â”œâ”€â”€ head1_planner.py         â† Generates analysis plan
      â”‚   â”‚   â”œâ”€â”€ head3_summarizer.py      â† Summarizes analytical results
      â”‚   â”‚   â””â”€â”€ selfCritique.py          â† Registry + dataset introspection
      â”‚   â””â”€â”€ intelligence/
      â”‚       â””â”€â”€ backend.py               â† FastAPI streaming backend
      â”‚
      â”œâ”€â”€ bharat-ui/                       â† React frontend (Vite + Tailwind)
      â”‚   â”œâ”€â”€ src/App.jsx                  â† Animated chat interface
      â”‚   â””â”€â”€ src/bharat.css               â† Obsidian-glass UI theme
      â””â”€â”€ models/
          â””â”€â”€ all-MiniLM-L6-v2             â† Local embedding model cache

---

## âš™ï¸ Setup & Installation

I have used  deadsnakes repo
```bash=
sudo add-apt-repository ppa:deadsnakes/nightly
sudo apt update
sudo apt install python3.10 python3.10-venv python3.10-distutils
```

then do 
```bash=
python3.10 -m venv .bharat
```

then 
```bash=
source .bharat/bin/activate
```

### 1ï¸âƒ£ Install dependencies

```bash=
pip install -r requirements.txt
```
---

### 2ï¸âƒ£ Install Ollama

Download from https://ollama.com/download  
Once installed, pull the required models:

      ollama pull mistral-nemo:12b
      ollama pull qwen2.5:7b
      ollama pull qwen2.5:14b
      ollama pull phi3:mini

ğŸ§  These models power the Head-1 Planner, Head-2 Executor, and Head-3 Summarizer modules.

---

## ğŸ§  SentenceTransformer Auto-Installer

In dataHandlers/llm_tools/embeddings.py (or wherever you initialize embeddings):

      from sentence_transformers import SentenceTransformer
      import os

      try:
          model_path = "./models/all-MiniLM-L6-v2"
          if not os.path.exists(model_path):
              raise FileNotFoundError

          self.sentence_model = SentenceTransformer(
              model_path,
              device="cpu",
              local_files_only=True
          )
      except Exception:
          print("âš ï¸ Local embedding model not found. Downloading...")
          self.sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
          os.makedirs("./models", exist_ok=True)
          self.sentence_model.save("./models/all-MiniLM-L6-v2")
          print("âœ… Model cached locally.")

This ensures your embedding model is automatically downloaded and cached once.

---

## ğŸš€ Running the System

### 1ï¸âƒ£ Start the backend
```bash=
in the Build_For_Bharat_Demo-main repository (while in venv do)
uvicorn intelligence.backend:app --reload --port 8000
sudo systemctl stop ollama.service
sudo systemctl disable ollama.service 
```
### 2ï¸âƒ£ Start Ollama
```bash=
ollama serve
```
### 3ï¸âƒ£ Start the frontend
```bash=
cd bharat-ui
npm install
npm run dev
```

### Note if your Node.js version is old <20.19 then update it using
```bash=
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt install -y nodejs
```

Then open http://localhost:5173

---

## ğŸ’¬ Query Flow

User enters:

      "Compare rainfall with wheat yield in Maharashtra between 2015â€“2020"

Backend pipeline executes:

      Head-1 Planner: Generate analytical plan
      Head-2 Executor: Run sequential dataset operations
      Head-3 Summarizer: Produce final insight

Frontend displays live streamed updates for each stage in translucent boxes.

---

## ğŸ§± Local-Only Policy

- All operations run entirely offline once datasets are cached.
- Datasets are fetched from public endpoints only (no scraping or login).
- Models are locally hosted through Ollama.
- No cloud dependency at any stage.

---

## ğŸª„ Roadmap

- Add charting and visualization in the UI (Plotly / Chart.js)
- Integrate voice input and speech summary
- Build agent registry for modular dataset families
- Add auto-dataset updater for new data.gov.in releases

---

## ğŸ Credits

Developed under the Build for Bharat Fellowship  
Leveraging open Indian datasets to enable transparent, sovereign AI.
